---
---

@inproceedings{arp-etal-2024-noncetree,
    title = {{Multilingual Nonce Dependency Treebanks: Understanding how LLMs represent and process syntactic structure}},
    author = "David Arps and Younes Samih and Laura Kallmeyer and Hassan Sajjad",
    booktitle = "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
    month = {June},
    year = "2024",
    address = "Mexico City, Mexico",
    url={https://arxiv.org/pdf/2311.07497.pdf},
}

@inproceedings{rosati-etal-2024-longform,
    title = {{Long-form Evaluation of Model Editing}},
    author = "Domenic Rosati and Robie Gonzales and Jinkun Chen and Xuemin Yu and Melis Erkan and Yahya Kayani and Satya Deepika Chavatapalli and Frank Rudzicz and Hassan Sajjad",
    booktitle = "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
    month = {June},
    year = "2024",
    address = "Mexico City, Mexico",
    url={https://arxiv.org/pdf/2402.09394.pdf},
}

@inproceedings{fan_neuroneval_neurips23,
title={Evaluating Neuron Interpretation Methods of {NLP} Models},
author={Yimin Fan and Fahim Dalvi and Nadir Durrani and Hassan Sajjad},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems (NeurIPs)},
year={2023},
url={https://openreview.net/forum?id=YiwMpyMdPX},
location = {New Orleans, US},
month={Dec},
keywords = {conference},
}

@InProceedings{yu_generalization_iclr2023,
  title={{Learning Uncertainty for Unknown Domains with Zero-Target-Assumption}},
  author={Yu Yu and Hassan Sajjad and Jia Xu},
booktitle={International Conference on Learning Representations (ICLR)},
year={2023},
location = {Kigali Rwanda},
keywords = {conference},
Month = {May},
url = {https://openreview.net/pdf?id=pWVASryOyFw},
}

@InProceedings{adverserial_acl23,
  title={{Impact of Adversarial Training on Robustness and Generalizability of Language Models}},
  author={Enes Altinisik and Hassan Sajjad and Husrev Taha Sencar and Safa Messaoud and Sanjay Chawla},
  booktitle={Proceedings of the Findings of Association for Computational Linguistics (ACL)},
  year={2023},
  location = {Toronto, Canada},
  month={Jul},
  url = {https://arxiv.org/abs/2211.05523},
} 

@inproceedings{sajjad-etal-2022-effect,
    title = "Effect of Post-processing on Contextualized Word Representations",
    author = "Sajjad, Hassan  and
      Alam, Firoj  and
      Dalvi, Fahim  and
      Durrani, Nadir",
    booktitle = "Proceedings of the 29th International Conference on Computational Linguistics (COLING)",
    month = {Oct},
    year = "2022",
    address = "Gyeongju, Republic of Korea",
    publisher = "International Committee on Computational Linguistics",
    url = {https://aclanthology.org/2022.coling-1.277},
    pages = "3127--3142",
}

@inproceedings{david2022_constituency,
title={{Probing for Constituency Structure in Neural Language Models}},
author={David Arps and Younes Samih and Laura Kallmeyer and Hassan Sajjad},
booktitle={Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
year={2022},
month={Dec},
address = "Abu Dhabi",
url={https://aclanthology.org/2022.findings-emnlp.502/},
}

@inproceedings{durrani2022_latent_ftmodels,
title={{On the Transformation of Latent Space in Fine-Tuned NLP Models}},
author={Nadir Durrani and Hassan Sajjad and Fahim Dalvi and Firoj Alam},
booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
year={2022},
month={Dec},
address = "Abu Dhabi",
url={https://aclanthology.org/2022.emnlp-main.97/},
}

@inproceedings{dalvi2022discovering,
title={{Discovering Latent Concepts Learned in BERT}},
author={Fahim Dalvi and Abdul Rafae Khan and Firoj Alam and Nadir Durrani and Jia Xu and Hassan Sajjad},
booktitle={International Conference on Learning Representations (ICLR)},
year={2022},
month={May},
address = "Online",
url={https://openreview.net/pdf?id=POTMtpYI1xH},
}

@inproceedings{sajjad-etal-2022-analyzing,
    title = {{Analyzing Encoded Concepts in Transformer Language Models}},
    author = "Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Alam, Firoj  and
      Khan, Abdul  and
      Xu, Jia",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)",
    month = {July},
    year = "2022",
    address = "Seattle, United States",
    url={https://aclanthology.org/2022.naacl-main.225/},
}

@InProceedings{sajjad_tutorial:naacl2021,
  title={{Fine-grained Interpretationand Causation Analysis in Deep NLP Models}},
  author={Hassan Sajjad and Narine Kokhlikyan and Fahim Dalvi and Nadir Durrani},
  booktitle={North American Chapter of the Association of Computational Linguistics: Human Language Technologies (NAACL-HLT)},
    year={2021},
      Month = {June},
    location = {Online},
    url={https://aclanthology.org/2021.naacl-tutorials.2/},
}

@InProceedings{durrani_finetuning:acl2021,
  title={{How Transfer Learning Impacts Linguistic Knowledge in Deep NLP Models?}},
  author={Nadir Durrani and Hassan Sajjad and Fahim Dalvi},
  booktitle={Findings of the Association for Computational Linguistics (ACL-IJCNLP))},
    year={2021},
      Month = {August},
    location = {Online},
    url={https://aclanthology.org/2021.findings-acl.438/},
}

@InProceedings{esther_causativity_iwcs21,
  title={{Implicit Representations of Event Properties within Contextual Language Models:
Searching for ``Causativity Neurons"}},
  author={Esther Seyffarth and  Younes Samih and Laura Kallmeyer and Hassan Sajjad},
  booktitle={International Conference on Computational Semantics (IWCS)},
    year={2021},
      Month = {June},
    location = {Groningen, Netherlands},
    url={https://aclanthology.org/2021.iwcs-1.11/},
}


@InProceedings{crisis_benchmarking:icwsm21,
  title={{CrisisBench: Benchmarking Crisis-related Social Media Datasets for Humanitarian Information Processing}},
  author={Firoj Alam and Hassan Sajjad and Muhammad Imran and Ferda Ofli},
  booktitle={International Conference on Web and Social Media (ICWSM)},
    year={2021},
      Month = {June},
    location = {Online},
    url = {https://arxiv.org/abs/2004.06774},
}

@InProceedings{firoj_covid:icwsm21,
  title={{Fighting the COVID-19 Infodemic in Social Media: A Holistic Perspective and a Call to Arms}},
  author={Firoj Alam and Fahim Dalvi and Shaden Shaar and Nadir Durrani and Hamdy Mubarak and Alex Nikolov and Giovanni Da San Martino and Ahmed Abdelali and Hassan Sajjad and Kareem Darwish and Preslav Nakov},
  booktitle={International Conference on Web and Social Media (ICWSM)},
  year={2021},
    Month = {June},
    location = {Online},
    url = {https://arxiv.org/abs/2007.07996}
} 

@inproceedings{suwaileh-etal-2020-ready,
    title = "Are We Ready for this Disaster? Towards Location Mention Recognition from Crisis Tweets",
    author = "Suwaileh, Reem  and
      Imran, Muhammad  and
      Elsayed, Tamer  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics (COLING)",
    month = {December},
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.550",
    pages = "6252--6263",
}

@inproceedings{sajjad-etal-2020-arabench,
    title = "{A}ra{B}ench: Benchmarking Dialectal {A}rabic-{E}nglish Machine Translation",
    author = "Sajjad, Hassan  and
      Abdelali, Ahmed  and
      Durrani, Nadir  and
      Dalvi, Fahim",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics (COLING)",
    month = {December},
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.447",
    pages = "5094--5107",
}

@InProceedings{durrani_individual:emnlp20,
  title={{Analyzing Individual Neurons in Pre-trained Language Models}},
  author={Nadir Durrani and Hassan Sajjad and Fahim Dalvi and Yonatan Belinkov},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020},
    Month = {November},
    location = {Online},
    url = {https://aclanthology.org/2020.emnlp-main.395.pdf}
} 

@InProceedings{dalvi_featureselection:emnlp20,
  title={{Analyzing Redundancy in Pretrained Transformer Models}},
  author={Fahim Dalvi and Hassan Sajjad and Nadir Durrani and Yonatan Belinkov},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2020},
    Month = {November},
    location = {Online},
    url = {https://aclanthology.org/2020.emnlp-main.398/}
} 

@InProceedings{wu_similarity:acl20,
  title={{Similarity Analysis of Contextual Word Representation Models}},
  author={John M. Wu* and Yonatan Belinkov* and Hassan Sajjad and  Nadir Durrani and Fahim Dalvi and James Glass},
  booktitle={Proceedings of the Annual Conference of the Association for Computational Linguistics (ACL)},
  Month ={July},
  year={2020},
  url = {https://aclanthology.org/2020.acl-main.422/}
} 

@InProceedings{durrani:2019:NAACL,
  title={{One Size Does Not Fit All: Comparing NMT Representations of Different Granularities}},
  author={Nadir Durrani and Fahim Dalvi and Hassan Sajjad and Yonatan Belinkov and Preslav Nakov},
  booktitle={Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
  year={2019},
    Month = {June},
    location = {Minneapolis, US},
    url = {https://aclanthology.org/N19-1154/}
} 

@InProceedings{mubarak:2019:NAACL,
  title={{Highly Effective Arabic Diacritization using Sequence to Sequence Modeling}},
  author={Hamdy Mubarak and Ahmed Abdelali and Hassan Sajjad and Younes Samih and Kareem Darwish},
  booktitle={Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
  year={2019},
    Month = {June},
    url = {https://aclanthology.org/N19-1248/}
}

@InProceedings{individual:iclr19,
  title={Identifying and Controlling Important Neurons in Neural Machine Translation},
  author={D. Anthony Bau* and Yonatan Belinkov* and Hassan Sajjad and Fahim Dalvi and Nadir Durrani and James Glass},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019},
 keywords = {conference},
 Month = {May},
 location = {New Orleans, US},
 url = {https://openreview.net/pdf?id=H1z-PsR5KX}
} 

@InProceedings{grain:aaai19-1,
  title={What is one Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models},
  author={Fahim Dalvi* and Nadir Durrani* and Hassan Sajjad* and Yonatan Belinkov  and D. Anthony Bau and James Glass},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},
  year={2019},
 keywords = {conference},
     Month = {March},
    location = {Honolulu, US},
    url = {https://arxiv.org/abs/1812.09355}

}

@InProceedings{dalvi:2018:NAACL,
  title={Incremental Decoding and Training Methods for Simultaneous Translation in Neural Machine Translation},
  author={Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan and Vogel, Stephan},
  booktitle={Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)},
  year={2018},
    Month = {June},
    location = {New Orleans, US},
    url = {https://aclanthology.org/N18-2079/}
} 


@InProceedings{belinkov:2017:ACL,
  title={What do Neural Machine Translation Models Learn about Morphology?},
  author={Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
  booktitle={Proceedings of the 55th Conference of the Association for Computational Linguistics (ACL)},
  year={2017},
    Month = {August},
    location = {Vancouver, Canada},
    url = {https://aclanthology.org/P17-1080/}
} 


@inproceedings{dalvi-etal-2017-understanding,
    title = "Understanding and Improving Morphological Learning in the Neural Machine Translation Decoder",
    author = "Dalvi, Fahim  and
      Durrani, Nadir  and
      Sajjad, Hassan  and
      Belinkov, Yonatan  and
      Vogel, Stephan",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1015",
    pages = "142--151",
    abstract = "End-to-end training makes the neural machine translation (NMT) architecture simpler, yet elegant compared to traditional statistical machine translation (SMT). However, little is known about linguistic patterns of morphology, syntax and semantics learned during the training of NMT systems, and more importantly, which parts of the architecture are responsible for learning each of these phenomenon. In this paper we i) analyze how much morphology an NMT decoder learns, and ii) investigate whether injecting target morphology in the decoder helps it to produce better translations. To this end we present three methods: i) simultaneous translation, ii) joint-data learning, and iii) multi-task learning. Our results show that explicit morphological information helps the decoder learn target language morphology and improves the translation quality by 0.2{--}0.6 BLEU points.",
}

@inproceedings{belinkov-etal-2017-evaluating,
    title = "Evaluating Layers of Representation in Neural Machine Translation on Part-of-Speech and Semantic Tagging Tasks",
    author = "Belinkov, Yonatan  and
      M{\`a}rquez, Llu{\'\i}s  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Glass, James",
    editor = "Kondrak, Greg  and
      Watanabe, Taro",
    booktitle = "Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = nov,
    year = "2017",
    address = "Taipei, Taiwan",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I17-1001",
    pages = "1--10",
    abstract = "While neural machine translation (NMT) models provide improved translation quality in an elegant framework, it is less clear what they learn about language. Recent work has started evaluating the quality of vector representations learned by NMT models on morphological and syntactic tasks. In this paper, we investigate the representations learned at different layers of NMT encoders. We train NMT systems on parallel data and use the models to extract features for training a classifier on two tasks: part-of-speech and semantic tagging. We then measure the performance of the classifier as a proxy to the quality of the original NMT model for the given task. Our quantitative analysis yields interesting insights regarding representation learning in NMT models. For instance, we find that higher layers are better at learning semantics while lower layers tend to be better for part-of-speech tagging. We also observe little effect of the target language on source-side representations, especially in higher quality models.",
}

@inproceedings{Sajjad:eacl09,
author = {Hassan Sajjad and Helmut Schmid},
title = {Tagging {Urdu} Text with Parts of Speech: A Tagger Comparison},
booktitle = {Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics (EACL)},
year = 2009,
month = {April},
location = {Athens, Greece},
}

@inproceedings{durrani-EtAl:2010:ACL,
author = {Durrani, Nadir and Sajjad, Hassan and Fraser, Alexander and Schmid, Helmut},
title = {{Hindi-to-Urdu} Machine Translation through Transliteration},
booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL)},
month = {July},
year = 2010,
location = {Uppsala, Sweden},
}

@Inproceedings{sajjad:coling2012,
author = {Sajjad, Hassan and Pantel, Patrick and Gamon, Michael},
title = {Underspecified Query Refinement via Natural Language Question Generation},
booktitle = {Proceedings of the 24th International Conference on Computational Linguistics (COLING)},
year = {2012},
month = {December},
location = {Mumbai, India},
}

@inproceedings{sajjad-EtAl:2011:IJCNLP-2011,
author = {Sajjad, Hassan and Durrani, Nadir and Schmid, Helmut and Fraser, Alexander},
title = {Comparing Two Techniques for Learning Transliteration Models Using a Parallel Corpus},
booktitle = {Proceedings of 5th International Joint Conference on Natural Language Processing (IJCNLP)},
month = {November},
year = {2011},
location = {Chiang Mai, Thailand},
}

@inproceedings{sajjad2013translating,
  title={{Translating Dialectal Arabic to English}},
  author={Sajjad, Hassan and Darwish, Kareem and Belinkov, Yonatan},
  booktitle={Proceedings of the 51st Conference of the Association for Computational Linguistics (ACL)},
  year={2013},
  month = {August},
location = {Sofia, Bulgaria},
}

@InProceedings{uddin:socialcom2014,
author = {Moeen Uddin, Mohammad and Imran, Mohammad and Sajjad, Hassan},
title = {Understanding Types of Users on {T}witter},
booktitle = {Proceedings of the 6th ASE International Conference in Social Computing (SocialCom)},
month = {May},
year = {2014}, 
location = {Stanford, USA},
}

@inproceedings{darwish2014verifiably,
  title={Verifiably Effective Arabic Dialect Identification.},
  author={Darwish, Kareem and Sajjad, Hassan and Mubarak, Hamdy},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2014},
    month={October},
  location={Doha, Qatar},
}

@inproceedings{rafae-etal-2015-unsupervised,
    title = "An Unsupervised Method for Discovering Lexical Variations in {R}oman {U}rdu Informal Text",
    author = "Rafae, Abdul  and
      Qayyum, Abdul  and
      Moeenuddin, Muhammad  and
      Karim, Asim  and
      Sajjad, Hassan  and
      Kamiran, Faisal",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1097",
    doi = "10.18653/v1/D15-1097",
    pages = "823--828",
}


@article{Nguyen_Ali Al Mannai_Joty_Sajjad_Imran_Mitra_2017, title={Robust Classification of Crisis-Related Data on Social Networks Using Convolutional Neural Networks}, volume={11}, url={https://ojs.aaai.org/index.php/ICWSM/article/view/14950}, DOI={10.1609/icwsm.v11i1.14950}, abstractNote={ &lt;p&gt; The role of social media, in particular microblogging platforms such as Twitter, as a conduit for actionable and tactical information during disasters is increasingly acknowledged. However, time-critical analysis of big crisis data on social media streams brings challenges to machine learning techniques, especially the ones that use supervised learning. The scarcity of labeled data, particularly in the early hours of a crisis, delays the learning process. Existing classification methods require a significant amount of labeled data specific to a particular event for training plus a lot of feature engineering to achieve best results. In this work, we introduce neural network based classification methods for identifying useful tweets during a crisis situation. At the onset of a disaster when no labeled data is available, our proposed method makes the best use of the out-of-event data and achieves good results. &lt;/p&gt; }, number={1}, journal={Proceedings of the International AAAI Conference on Web and Social Media}, author={Nguyen, Dat and Ali Al Mannai, Kamela and Joty, Shafiq and Sajjad, Hassan and Imran, Muhammad and Mitra, Prasenjit}, year={2017}, month={May}, pages={632-635} }

@inproceedings{sajjad-etal-2017-challenging,
    title = "Challenging Language-Dependent Segmentation for {A}rabic: An Application to Machine Translation and Part-of-Speech Tagging",
    author = "Sajjad, Hassan  and
      Dalvi, Fahim  and
      Durrani, Nadir  and
      Abdelali, Ahmed  and
      Belinkov, Yonatan  and
      Vogel, Stephan",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-2095",
    doi = "10.18653/v1/P17-2095",
    pages = "601--607",
    abstract = "Word segmentation plays a pivotal role in improving any Arabic NLP application. Therefore, a lot of research has been spent in improving its accuracy. Off-the-shelf tools, however, are: i) complicated to use and ii) domain/dialect dependent. We explore three language-independent alternatives to morphological segmentation using: i) data-driven sub-word units, ii) characters as a unit of learning, and iii) word embeddings learned using a character CNN (Convolution Neural Network). On the tasks of Machine Translation and POS tagging, we found these methods to achieve close to, and occasionally surpass state-of-the-art performance. In our analysis, we show that a neural machine translation system is sensitive to the ratio of source and target tokens, and a ratio close to 1 or greater, gives optimal performance.",
}

@inproceedings{durrani-etal-2016-deep,
    title = "A Deep Fusion Model for Domain Adaptation in Phrase-based {MT}",
    author = "Durrani, Nadir  and
      Sajjad, Hassan  and
      Joty, Shafiq  and
      Abdelali, Ahmed",
    editor = "Matsumoto, Yuji  and
      Prasad, Rashmi",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/C16-1299",
    pages = "3177--3187",
    abstract = "We present a novel fusion model for domain adaptation in Statistical Machine Translation. Our model is based on the joint source-target neural network Devlin et al., 2014, and is learned by fusing in- and out-domain models. The adaptation is performed by backpropagating errors from the output layer to the word embedding layer of each model, subsequently adjusting parameters of the composite model towards the in-domain data. On the standard tasks of translating English-to-German and Arabic-to-English TED talks, we observed average improvements of +0.9 and +0.7 BLEU points, respectively over a competition grade phrase-based system. We also demonstrate improvements over existing adaptation methods.",
}

@inproceedings{sajjad-etal-2016-eyes,
    title = "Eyes Don{'}t Lie: Predicting Machine Translation Quality Using Eye Movement",
    author = "Sajjad, Hassan  and
      Guzm{\'a}n, Francisco  and
      Durrani, Nadir  and
      Abdelali, Ahmed  and
      Bouamor, Houda  and
      Temnikova, Irina  and
      Vogel, Stephan",
    editor = "Knight, Kevin  and
      Nenkova, Ani  and
      Rambow, Owen",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N16-1125",
    doi = "10.18653/v1/N16-1125",
    pages = "1082--1088",
}

@inproceedings{sajjad-etal-2016-empirical,
    title = "An Empirical Study: Post-editing Effort for {E}nglish to {A}rabic Hybrid Machine Translation",
    author = "Sajjad, Hassan  and
      Guzman, Francisco  and
      Vogel, Stephan",
    editor = "Green, Spence  and
      Schwartz, Lane",
    booktitle = "Conferences of the Association for Machine Translation in the Americas: MT Users' Track",
    month = oct # " 28 - " # nov # " 1",
    year = "2016",
    address = "Austin, TX, USA",
    publisher = "The Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2016.amta-users.9",
    pages = "92--113",
}


@InProceedings{durrani-EtAl:2014:EACL,
  author    = {Durrani, Nadir  and  Sajjad, Hassan  and  Hoang,  Hieu  and  Koehn, Philipp},
  title     = "{Integrating an Unsupervised Transliteration Model into Statistical Machine Translation}",
booktitle = {Proceedings of the 15th Conference of the European Chapter of the ACL (EACL)},
  month     = {April},
  year      = {2014},
  address   = {Gothenburg, Sweden},
}

@InProceedings{Abdelali_2014_lrec, 
author = {Ahmed Abdelali and Francisco Guzman and Hassan Sajjad and Stephan Vogel}, 
title = {The {AMARA} Corpus: Building Parallel Language Resources for the Educational Domain}, 
booktitle = {Proceedings of the 9th International Conference on Language Resources and Evaluation (LREC)}, 
year = {2014}, 
month = {May}, 
address = {Reykjavik, Iceland}, 
}

@inproceedings{durrani-etal-2015-using,
    title = "Using joint models or domain adaptation in statistical machine translation",
    author = "Durrani, Nadir  and
      Sajjad, Hassan  and
      Joty, Shafiq  and
      Abdelali, Ahmed  and
      Vogel, Stephan",
    booktitle = "Proceedings of Machine Translation Summit XV: Papers",
    month = oct # " 30 {--} " # nov # " 3",
    year = "2015",
    address = "Miami, USA",
    url = "https://aclanthology.org/2015.mtsummit-papers.10",
}

@inproceedings{magdy2015distant,
  title={Distant Supervision for Tweet Classification Using YouTube Labels},
  author={Magdy, Walid and Sajjad, Hassan and El-Ganainy, Tarek and Sebastiani, Fabrizio},
  booktitle={Proceedings of the Ninth International AAAI Conference on Web and Social Media (ICWSM)},
  location={Oxford, UK},
  month ={May},
  year={2015},
}

@inproceedings{joty-etal-2015-avoid,
    title = "How to Avoid Unwanted Pregnancies: Domain Adaptation using Neural Network Models",
    author = "Joty, Shafiq  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Al-Mannai, Kamla  and
      Abdelali, Ahmed  and
      Vogel, Stephan",
    editor = "M{\`a}rquez, Llu{\'\i}s  and
      Callison-Burch, Chris  and
      Su, Jian",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D15-1147",
    doi = "10.18653/v1/D15-1147",
    pages = "1259--1270",
}


@InProceedings{sajjad:acl12,
  author    = {Hassan Sajjad and Alexander Fraser and Helmut Schmid},
  title     = {A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining},
  BOOKTITLE = {Proceedings of the 50th Conference of the Association for Computational Linguistics (ACL)},
  year      = {2012},
    Month = {July},
  address   = {Jeju, Korea},
}

@InProceedings{sajjad:acl11,
  author    = {Hassan Sajjad and Alexander Fraser and Helmut Schmid},
  title     = {An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment},
  booktitle = {Proceedings of the 49th Conference of the Association for Computational Linguistics: Human Language Technologies (ACL-HLT)},
  year      = {2011},
  Month = {June},
  address   = {Portland, OR, USA},
}

@article{10.5555/3648699.3649061,
author = {Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan},
title = {Discovering salient neurons in deep NLP models},
year = {2024},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {While a lot of work has been done in understanding representations learned within deep NLP models and what knowledge they capture, work done towards analyzing individual neurons is relatively sparse. We present a technique called Linguistic Correlation Analysis to extract salient neurons in the model, with respect to any extrinsic property, with the goal of understanding how such knowledge is preserved within neurons. We carry out a fine-grained analysis to answer the following questions: (i) can we identify subsets of neurons in the network that learn a specific linguistic property? (ii) is a certain linguistic phenomenon in a given model localized (encoded in few individual neurons) or distributed across many neurons? (iii) how redundantly is the information preserved? (iv) how does fine-tuning pre-trained models towards downstream NLP tasks impact the learned linguistic knowledge? (v) how do models vary in learning different linguistic properties? Our data-driven, quantitative analysis illuminates interesting findings: (i) we found small subsets of neurons that can predict different linguistic tasks; (ii) neurons capturing basic lexical information, such as suffixation, are localized in the lowermost layers; (iii) neurons learning complex concepts, such as syntactic role, are predominantly found in middle and higher layers; (iv) salient linguistic neurons are relocated from higher to lower layers during transfer learning, as the network preserves the higher layers for task-specific information; (v) we found interesting differences across pre-trained models regarding how linguistic information is preserved within them; and (vi) we found that concepts exhibit similar neuron distribution across different languages in the multilingual transformer models. Our code is publicly available as part of the NeuroX toolkit (Dalvi et al., 2023). https://github.com/fdalvi/NeuroX},
journal = {J. Mach. Learn. Res.},
month = mar,
articleno = {362},
numpages = {40},
url = {https://www.jmlr.org/papers/volume24/23-0074/23-0074.pdf},
keywords = {neuron analysis, representation analysis, interpretability, explainable AI}
}

@article{SUWAILEH2022103107,
title = {When a disaster happens, we are ready: Location mention recognition from crisis tweets},
journal = {International Journal of Disaster Risk Reduction},
volume = {78},
pages = {103107},
year = {2022},
month = {June},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2022.103107},
url = {https://www.sciencedirect.com/science/article/pii/S2212420922003260},
author = {Reem Suwaileh and Tamer Elsayed and Muhammad Imran and Hassan Sajjad},
keywords = {Geolocation recognition, Social good, Twitter},
abstract = {Geolocation information is important for humanitarian organizations to gain situational awareness and deliver timely aid during disasters. Towards addressing the problem of recognizing locations, i.e., Location Mention Recognition (LMR), within social media posts during disasters, past studies mainly focused on proposing techniques that assume the availability of abundant training data at the disaster onset. In this work, we adopt the more realistic assumption that no (i.e., zero-shot setting) or as little as a few hundred examples (i.e., few-shot setting) from the just-occurred event is available for training. Specifically, we examine the effect of training a BERT-based LMR model on past events using different settings, datasets, languages, and geo-proximity. Extensive empirical analysis provides several insights for building an effective LMR model during disasters, including (i) Twitter crisis-related and location-specific data from geographically-nearby disaster events is more useful than all other combinations of training datasets in the zero-shot monolingual setting, (ii) using as few as 263–356 training tweets from the target language (i.e., few-shot setting) remarkably boosts the performance in the cross- and multilingual settings, and (iii) labeling about 500 target event's tweets leads to an acceptable LMR performance, higher than F1 of 0.7, in the monolingual settings. Finally, we conduct an extensive error analysis and highlight issues related to the quality of the available datasets and weaknesses of the current model.}
}

@article{10.1162/tacl_a_00519,
    author = {Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim},
    title = "{Neuron-level Interpretation of Deep NLP Models: A Survey}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {10},
    pages = {1285-1303},
    year = {2022},
    month = {11},
    abstract = "{The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00519},
    url = {https://doi.org/10.1162/tacl\_a\_00519},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00519/2060745/tacl\_a\_00519.pdf},
}


@article{SAJJAD2023101429,
title = {On the effect of dropping layers of pre-trained transformer models},
journal = {Computer Speech & Language},
volume = {77},
pages = {101429},
year = {2023},
month = {July},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2022.101429},
url = {https://www.sciencedirect.com/science/article/pii/S0885230822000596},
author = {Hassan Sajjad and Fahim Dalvi and Nadir Durrani and Preslav Nakov},
keywords = {Pre-trained transformer models, Efficient transfer learning, Interpretation and analysis},
abstract = {Transformer-based NLP models are trained using hundreds of millions or even billions of parameters, limiting their applicability in computationally constrained environments. While the number of parameters generally correlates with performance, it is not clear whether the entire network is required for a downstream task. Motivated by the recent work on pruning and distilling pre-trained models, we explore strategies to drop layers in pre-trained models, and observe the effect of pruning on downstream GLUE tasks. We were able to prune BERT, RoBERTa and XLNet models up to 40%, while maintaining up to 98% of their original performance. Additionally we show that our pruned models are on par with those built using knowledge distillation, both in terms of size and performance. Our experiments yield interesting observations such as: (i) the lower layers are most critical to maintain downstream task performance, (ii) some tasks such as paraphrase detection and sentence similarity are more robust to the dropping of layers, and (iii) models trained using different objective function exhibit different learning patterns and w.r.t the layer dropping.}
}

@article{ganesh-etal-2021-compressing,
    title = "Compressing Large-Scale Transformer-Based Models: A Case Study on {BERT}",
    author = "Ganesh, Prakhar  and
      Chen, Yao  and
      Lou, Xin  and
      Khan, Mohammad Ali  and
      Yang, Yin  and
      Sajjad, Hassan  and
      Nakov, Preslav  and
      Chen, Deming  and
      Winslett, Marianne",
    editor = "Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    month = {September},
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.63",
    doi = "10.1162/tacl_a_00413",
    pages = "1061--1080",
    abstract = "Pre-trained Transformer-based models have achieved state-of-the-art performance for various Natural Language Processing (NLP) tasks. However, these models often have billions of parameters, and thus are too resource- hungry and computation-intensive to suit low- capability devices or applications with strict latency requirements. One potential remedy for this is model compression, which has attracted considerable research attention. Here, we summarize the research in compressing Transformers, focusing on the especially popular BERT model. In particular, we survey the state of the art in compression for BERT, we clarify the current best practices for compressing large-scale Transformer models, and we provide insights into the workings of various methods. Our categorization and analysis also shed light on promising future research directions for achieving lightweight, accurate, and generic NLP models.",
}

@article{rafae2018:CL,
  title={A Clustering Framework for Lexical Normalization of Roman {Urdu}},
  author={Rafae, Abdul and Karim, Asim and Sajjad, Hassan and Kamiran, Faisal and Jia Xu},
  Journal={Natural Language Engineering (NLE)},
  year = {2020},
  month = {March},
  url = {https://doi.org/10.1017/S1351324920000285},
 keywords = {journal},
}

@article{madgy2015:SNAM,
 journal = {Social Network Analysis and Mining},
 title = {Bridging social media via distant supervision},
 author = {Magdy, Walid and Sajjad, Hassan and El-Ganainy, Tarek and Sebastiani, Fabrizio},
 year = {2015},
 volume = {35},
 number = {5},
}

@article{JOTY2017161,
title = {Domain adaptation using neural network joint model},
journal = {Computer Speech & Language},
volume = {45},
pages = {161-179},
year = {2017},
month = {September},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2016.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0885230816301474},
author = {Shafiq Joty and Nadir Durrani and Hassan Sajjad and Ahmed Abdelali},
keywords = {Machine translation, Domain adaptation, Neural network joint model, Distributed representation of texts, Noise contrastive estimation},
abstract = {We explore neural joint models for the task of domain adaptation in machine translation in two ways: (i) we apply state-of-the-art domain adaptation techniques, such as mixture modelling and data selection using the recently proposed Neural Network Joint Model (NNJM) (Devlin et al., 2014); (ii) we propose two novel approaches to perform adaptation through instance weighting and weight readjustment in the NNJM framework. In our first approach, we propose a pair of models called Neural Domain Adaptation Models (NDAM) that minimizes the cross entropy by regularizing the loss function with respect to in-domain (and optionally to out-domain) model. In the second approach, we present a set of Neural Fusion Models (NFM) that combines the in- and the out-domain models by readjusting their parameters based on the in-domain data. We evaluated our models on the standard task of translating English-to-German and Arabic-to-English TED talks. The NDAM models achieved better perplexities and modest BLEU improvements compared to the baseline NNJM, trained either on in-domain or on a concatenation of in- and out-domain data. On the other hand, the NFM models obtained significant improvements of up to +0.9 and +0.7 BLEU points, respectively. We also demonstrate improvements over existing adaptation methods such as instance weighting, phrasetable fill-up, linear and log-linear interpolations.}
}

@article{sajjad-etal-2017-statistical,
    title = "Statistical Models for Unsupervised, Semi-Supervised Supervised Transliteration Mining",
    author = {Sajjad, Hassan  and
      Schmid, Helmut  and
      Fraser, Alexander  and
      Sch{\"u}tze, Hinrich},
    journal = "Computational Linguistics",
    volume = "43",
    number = "2",
    month = jun,
    year = "2017",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J17-2003",
    doi = "10.1162/COLI_a_00286",
    pages = "349--375",
    abstract = "We present a generative model that efficiently mines transliteration pairs in a consistent fashion in three different settings: unsupervised, semi-supervised, and supervised transliteration mining. The model interpolates two sub-models, one for the generation of transliteration pairs and one for the generation of non-transliteration pairs (i.e., noise). The model is trained on noisy unlabeled data using the EM algorithm. During training the transliteration sub-model learns to generate transliteration pairs and the fixed non-transliteration model generates the noise pairs. After training, the unlabeled data is disambiguated based on the posterior probabilities of the two sub-models. We evaluate our transliteration mining system on data from a transliteration mining shared task and on parallel corpora. For three out of four language pairs, our system outperforms all semi-supervised and supervised systems that participated in the NEWS 2010 shared task. On word pairs extracted from parallel corpora with fewer than 2{\%} transliteration pairs, our system achieves up to 86.7{\%} F-measure with 77.9{\%} precision and 97.8{\%} recall.",
}

@article{10.1162/coli_a_00367,
    author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
    title = "{On the Linguistic Representational Power of Neural Machine Translation Models}",
    journal = {Computational Linguistics},
    volume = {46},
    number = {1},
    pages = {1-52},
    year = {2020},
    month = {03},
    abstract = "{Despite the recent success of deep neural networks in natural language processing and other spheres of artificial intelligence, their interpretability remains a challenge. We analyze the representations learned by neural machine translation (NMT) models at various levels of granularity and evaluate their quality through relevant extrinsic properties. In particular, we seek answers to the following questions: (i) How accurately is word structure captured within the learned representations, which is an important aspect in translating morphologically rich languages? (ii) Do the representations capture long-range dependencies, and effectively handle syntactically divergent languages? (iii) Do the representations capture lexical semantics? We conduct a thorough investigation along several parameters: (i) Which layers in the architecture capture each of these linguistic phenomena; (ii) How does the choice of translation unit (word, character, or subword unit) impact the linguistic properties captured by the underlying representations? (iii) Do the encoder and decoder learn differently and independently? (iv) Do the representations learned by multilingual NMT models capture the same amount of linguistic information as their bilingual counterparts? Our data-driven, quantitative evaluation illuminates important aspects in NMT models and their ability to capture various linguistic phenomena. We show that deep NMT models trained in an end-to-end fashion, without being provided any direct supervision during the training process, learn a non-trivial amount of linguistic information. Notable findings include the following observations: (i) Word morphology and part-of-speech information are captured at the lower layers of the model; (ii) In contrast, lexical semantics or non-local syntactic and semantic dependencies are better represented at the higher layers of the model; (iii) Representations learned using characters are more informed about word-morphology compared to those learned using subword units; and (iv) Representations learned by multilingual models are richer compared to bilingual models.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00367},
    url = {https://doi.org/10.1162/coli\_a\_00367},
    eprint = {https://direct.mit.edu/coli/article-pdf/46/1/1/1847791/coli\_a\_00367.pdf},
}


@inproceedings{abdelali-etal-2022-post,
    title = "Post-hoc analysis of {A}rabic transformer models",
    author = "Abdelali, Ahmed  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Sajjad, Hassan",
    editor = "Bastings, Jasmijn  and
      Belinkov, Yonatan  and
      Elazar, Yanai  and
      Hupkes, Dieuwke  and
      Saphra, Naomi  and
      Wiegreffe, Sarah",
    booktitle = "Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates (Hybrid)",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.blackboxnlp-1.8",
    doi = "10.18653/v1/2022.blackboxnlp-1.8",
    pages = "91--103",
    abstract = "Arabic is a Semitic language which is widely spoken with many dialects. Given the success of pre-trained language models, many transformer models trained on Arabic and its dialects have surfaced. While there have been an extrinsic evaluation of these models with respect to downstream NLP tasks, no work has been carried out to analyze and compare their internal representations. We probe how linguistic information is encoded in the transformer models, trained on different Arabic dialects. We perform a layer and neuron analysis on the models using morphological tagging tasks for different dialects of Arabic and a dialectal identification task. Our analysis enlightens interesting findings such as: i) word morphology is learned at the lower and middle layers, ii) while syntactic dependencies are predominantly captured at the higher layers, iii) despite a large overlap in their vocabulary, the MSA-based models fail to capture the nuances of Arabic dialects, iv) we found that neurons in embedding layers are polysemous in nature, while the neurons in middle layers are exclusive to specific properties.",
}

@inproceedings{specia-etal-2020-findings,
    title = "Findings of the {WMT} 2020 Shared Task on Machine Translation Robustness",
    author = "Specia, Lucia  and
      Li, Zhenhao  and
      Pino, Juan  and
      Chaudhary, Vishrav  and
      Guzm{\'a}n, Francisco  and
      Neubig, Graham  and
      Durrani, Nadir  and
      Belinkov, Yonatan  and
      Koehn, Philipp  and
      Sajjad, Hassan  and
      Michel, Paul  and
      Li, Xian",
    editor = {Barrault, Lo{\"\i}c  and
      Bojar, Ond{\v{r}}ej  and
      Bougares, Fethi  and
      Chatterjee, Rajen  and
      Costa-juss{\`a}, Marta R.  and
      Federmann, Christian  and
      Fishel, Mark  and
      Fraser, Alexander  and
      Graham, Yvette  and
      Guzman, Paco  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Morishita, Makoto  and
      Monz, Christof  and
      Nagata, Masaaki  and
      Nakazawa, Toshiaki  and
      Negri, Matteo},
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.wmt-1.4",
    pages = "76--91",
    abstract = "We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs {--} English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for {''}catastrophic errors{''}. We received 59 submissions by 11 participating teams from a variety of types of institutions.",
}

@inproceedings{li-etal-2019-findings,
    title = "Findings of the First Shared Task on Machine Translation Robustness",
    author = "Li, Xian  and
      Michel, Paul  and
      Anastasopoulos, Antonios  and
      Belinkov, Yonatan  and
      Durrani, Nadir  and
      Firat, Orhan  and
      Koehn, Philipp  and
      Neubig, Graham  and
      Pino, Juan  and
      Sajjad, Hassan",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajen  and
      Federmann, Christian  and
      Fishel, Mark  and
      Graham, Yvette  and
      Haddow, Barry  and
      Huck, Matthias  and
      Yepes, Antonio Jimeno  and
      Koehn, Philipp  and
      Martins, Andr{\'e}  and
      Monz, Christof  and
      Negri, Matteo  and
      N{\'e}v{\'e}ol, Aur{\'e}lie  and
      Neves, Mariana  and
      Post, Matt  and
      Turchi, Marco  and
      Verspoor, Karin",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5303",
    doi = "10.18653/v1/W19-5303",
    pages = "91--102",
    abstract = "We share the findings of the first shared task on improving robustness of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models{'} robustness to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson{'}s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task. Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment.",
}

@InProceedings{zaghouani2016normalizing,
  title={Normalizing Mathematical Expressions to Improve the Translation of Educational Content},
  author={Zaghouani, Wajdi and Abdelali, Ahmed and Guzm{\'a}n, Francisco and Sajjad, Hassan},
  booktitle={Proceedings of the AMTA 2016 Workshop Semitic Machine Translation (SeMaT)},
  location = {Austin, US},
  month={October},
  year={2016},
}

@inproceedings{DBLP:conf/lrec/BouamorS18,
  author       = {Houda Bouamor and
                  Hassan Sajjad},
  editor       = {Reinhard Rapp and
                  Pierre Zweigenbaum and
                  Serge Sharoff},
  title        = {H2@BUCC18: Parallel Sentence Extraction from Comparable Corpora Using
                  Multilingual Sentence Embeddings},
  booktitle    = {11th Workshop on Building and Using Comparable Corpora - Special Topic:
                  Comparable Corpora for Asian Languages, BUCC@LREC 2018, Miyazaki,
                  Japan, May 8, 2018},
  publisher    = {European Language Resources Association},
  year         = {2018},
  url          = {http://lrec-conf.org/workshops/lrec2018/W8/summaries/8\_W8.html},
  timestamp    = {Fri, 15 Sep 2023 14:10:05 +0200},
  biburl       = {https://dblp.org/rec/conf/lrec/BouamorS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sajjad-etal-2017-neural,
    title = "Neural Machine Translation Training in a Multi-Domain Scenario",
    author = "Sajjad, Hassan  and
      Durrani, Nadir  and
      Dalvi, Fahim  and
      Belinkov, Yonatan  and
      Vogel, Stephan",
    editor = "Sakti, Sakriani  and
      Utiyama, Masao",
    booktitle = "Proceedings of the 14th International Conference on Spoken Language Translation",
    month = dec # " 14-15",
    year = "2017",
    address = "Tokyo, Japan",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2017.iwslt-1.10",
    pages = "66--73",
    abstract = "In this paper, we explore alternative ways to train a neural machine translation system in a multi-domain scenario. We investigate data concatenation (with fine tuning), model stacking (multi-level fine tuning), data selection and multi-model ensemble. Our findings show that the best translation quality can be achieved by building an initial system on a concatenation of available out-of-domain data and then fine-tuning it on in-domain data. Model stacking works best when training begins with the furthest out-of-domain data and the model is incrementally fine-tuned with the next furthest domain and so on. Data selection did not give the best results, but can be considered as a decent compromise between training time and translation quality. A weighted ensemble of different individual models performed better than data selection. It is beneficial in a scenario when there is no time for fine-tuning an already trained model.",
}


@inproceedings{kamla:anlp2014,
  title={Unsupervised word segmentation improves dialectal {Arabic to English} machine translation},
  author={Al-Mannai, Kamla and Sajjad, Hassan and Khader, Alaa and Al Obaidli, Fahad and Nakov, Preslav and Vogel, Stephan},
  booktitle={Proceedings of the Workshop of Arabic Natural Language Processing (ANLP)},
  year={2014},
  month={October},
  location={Doha, Qatar},
 
}

@inproceedings{weller13:wmt13,
author = {Marion Weller and Max Kisselew and Svetlana Smekalova and Alexander Fraser and Helmut Schmid and Nadir Durrani and Hassan Sajjad and Richárd Farkas},
title = {{Munich-Edinburgh-Stuttgart Submissions at WMT13: Morphological and Syntactic Processing for SMT}},
booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT)},
year = 2013,
location = {Sofia, Bulgaria},
month = {August},
}

@Inproceedings{durrani-EtAl:2013:WMT,
author = {Nadir Durrani and Helmut Schmid and Alexander Fraser and Hassan Sajjad and Richárd Farkas},
title = {{Munich-Edinburgh-Stuttgart Submissions of OSM Systems at WMT13}},
booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT)},
year = {2013},
month = {August},
location = {Sofia, Bulgaria},
 
}

@Inproceedings{sajjad-EtAl:2013:WMT,
author = {Sajjad, Hassan and Smekalova, Svetlana and Durrani, Nadir and Fraser, Alexander and Schmid, Helmut},
title = {{QCRI-MES} Submission at {WMT}13: Using Transliteration Mining to Improve Statistical Machine Translation},
booktitle = {Proceedings of the Eighth Workshop on Statistical Machine Translation (WMT)},
year = {2013},
month = {August},
location = {Sofia, Bulgaria},
 
}

@inproceedings{bouamor-etal-2015-qcmuq,
    title = "{QCMUQ}@{QALB}-2015 Shared Task: Combining Character level {MT} and Error-tolerant Finite-State Recognition for {A}rabic Spelling Correction",
    author = "Bouamor, Houda  and
      Sajjad, Hassan  and
      Durrani, Nadir  and
      Oflazer, Kemal",
    editor = "Habash, Nizar  and
      Vogel, Stephan  and
      Darwish, Kareem",
    booktitle = "Proceedings of the Second Workshop on {A}rabic Natural Language Processing",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3217",
    doi = "10.18653/v1/W15-3217",
    pages = "144--149",
}

@inproceedings{guzman-etal-2015-humans,
    title = "How do Humans Evaluate Machine Translation",
    author = "Guzm{\'a}n, Francisco  and
      Abdelali, Ahmed  and
      Temnikova, Irina  and
      Sajjad, Hassan  and
      Vogel, Stephan",
    editor = "Bojar, Ond{\v{r}}ej  and
      Chatterjee, Rajan  and
      Federmann, Christian  and
      Haddow, Barry  and
      Hokamp, Chris  and
      Huck, Matthias  and
      Logacheva, Varvara  and
      Pecina, Pavel",
    booktitle = "Proceedings of the Tenth Workshop on Statistical Machine Translation",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-3059",
    doi = "10.18653/v1/W15-3059",
    pages = "457--466",
}


@InProceedings{sajjad:nist15,
  title={{QCN Egyptian Arabic to English Machine
Translation System for NIST OpenMT15}},
  author={Hassan Sajjad and Nadir Durrani and Francisco Guzman and Preslav Nakov and Ahmed Abdelali and  Stephan Vogel and
Wael Salloum and Ahmed El Kholy and Nizar Habash},
  booktitle={Workshop of NIST OpenMT15},
  location = {Washington DC, US},
  month={June},
  year={2015},
}

@InProceedings{nguyen2016:swdm,
  title={Applications of Online Deep Learning for Crisis Response Using Social Media Information},
  author={Dat Tien Nguyen and Shafiq Joty and Muhammad Imran and Hassan Sajjad and Prasenjit Mitra},
  booktitle={Proceedings of the 4th International Workshop on Social Web for Disaster Management (SWDM)},
  location = {Indianapolis, US},
  month={October},
  year={2016},
}

@inproceedings{eldesouki-etal-2016-qcri,
    title = "{QCRI} @ {DSL} 2016: Spoken {A}rabic Dialect Identification Using Textual Features",
    author = "Eldesouki, Mohamed  and
      Dalvi, Fahim  and
      Sajjad, Hassan  and
      Darwish, Kareem",
    editor = {Nakov, Preslav  and
      Zampieri, Marcos  and
      Tan, Liling  and
      Ljube{\v{s}}i{\'c}, Nikola  and
      Tiedemann, J{\"o}rg  and
      Malmasi, Shervin},
    booktitle = "Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://aclanthology.org/W16-4828",
    pages = "221--226",
    abstract = "The paper describes the QCRI submissions to the task of automatic Arabic dialect classification into 5 Arabic variants, namely Egyptian, Gulf, Levantine, North-African, and Modern Standard Arabic (MSA). The training data is relatively small and is automatically generated from an ASR system. To avoid over-fitting on such small data, we carefully selected and designed the features to capture the morphological essence of the different dialects. We submitted four runs to the Arabic sub-task. For all runs, we used a combined feature vector of character bi-grams, tri-grams, 4-grams, and 5-grams. We tried several machine-learning algorithms, namely Logistic Regression, Naive Bayes, Neural Networks, and Support Vector Machines (SVM) with linear and string kernels. However, our submitted runs used SVM with a linear kernel. In the closed submission, we got the best accuracy of 0.5136 and the third best weighted F1 score, with a difference less than 0.002 from the highest score.",
}

@inproceedings{durrani-etal-2016-qcris,
    title = "{QCRI}{'}s Machine Translation Systems for {IWSLT}{'}16",
    author = "Durrani, Nadir  and
      Dalvi, Fahim  and
      Sajjad, Hassan  and
      Vogel, Stephan",
    editor = {Cettolo, Mauro  and
      Niehues, Jan  and
      St{\"u}ker, Sebastian  and
      Bentivogli, Luisa  and
      Cattoni, Rolando  and
      Federico, Marcello},
    booktitle = "Proceedings of the 13th International Conference on Spoken Language Translation",
    month = dec # " 8-9",
    year = "2016",
    address = "Seattle, Washington D.C",
    publisher = "International Workshop on Spoken Language Translation",
    url = "https://aclanthology.org/2016.iwslt-1.18",
    abstract = "This paper describes QCRI{'}s machine translation systems for the IWSLT 2016 evaluation campaign. We participated in the Arabic→English and English→Arabic tracks. We built both Phrase-based and Neural machine translation models, in an effort to probe whether the newly emerged NMT framework surpasses the traditional phrase-based systems in Arabic-English language pairs. We trained a very strong phrase-based system including, a big language model, the Operation Sequence Model, Neural Network Joint Model and Class-based models along with different domain adaptation techniques such as MML filtering, mixture modeling and using fine tuning over NNJM model. However, a Neural MT system, trained by stacking data from different genres through fine-tuning, and applying ensemble over 8 models, beat our very strong phrase-based system by a significant 2 BLEU points margin in Arabic→English direction. We did not obtain similar gains in the other direction but were still able to outperform the phrase-based system. We also applied system combination on phrase-based and NMT outputs.",
}


@InProceedings{guzman-sajjad-etal:iwslt13,
author = {Guzm{\'a}n, Francisco and Sajjad, Hassan and Vogel, Stephan and Abdelali, Ahmed},
title = {The {AMARA} Corpus: Building Resources for Translating the Web's Educational Content},
booktitle = {Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT)},
month = {December},
year = {2013}, 
location = {Heidelberg, Germany}

}


@InProceedings{sajjad-etal:iwslt13,
author = {Hassan Sajjad and Francisco Guzmán and Preslav Nakov and Ahmed Abdelali and Kenton Murray and Fahad Al Obaidli and Stephan Vogel},
title = {{QCRI} at {IWSLT} 2013: Experiments in {Arabic-English and English-Arabic} Spoken Language Translation},
booktitle = {Proceedings of the 10th International Workshop on Spoken Language Technology (IWSLT)},
month = {December},
year = {2013},
location = {Heidelberg, Germany}

}

@InProceedings{neurox-acl23:demo,
  title={{NeuroX Library for Neuron Analysis of Deep NLP Models}},
  author={Fahim Dalvi and Hassan Sajjad and and Nadir Durrani},
  booktitle={Proceedings of the Association for Computational Linguistics (ACL)},
  year={2023},
  location = {Toronto, Canada},
  month={Jul},
  url = {https://youtu.be/mLhs2YMx4u8}
} 

@InProceedings{nxplain-eacl23:demo,
  title={{NxPlain: A Web-based Tool for Discovery of Latent Concepts}},
  author={Fahim Dalvi and Nadir Durrani and Hassan Sajjad and Tamim Jaban and Mus'ab Husaini and Ummar Abbas},
  booktitle={Proceedings of the European Chapter of the Association for Computational Linguistics (EACL)},
  year={2023},
  location = {Dubrovnik, Croatia},
  month={May},
  url = {https://aclanthology.org/2023.eacl-demo.10/}
} 


@InProceedings{conceptx-aaai23:demo,
  title={{ConceptX: A Framework for Latent Concept Analysis}},
  author={Firoj Alam and Fahim Dalvi and Nadir Durrani and Hassan Sajjad and Abdul Rafae Khan and Jia Xu},
  booktitle={AAAI Conference on Artificial Intelligence (AAAI)},
  year={2023},
  location = {Washington DC, USA},
  month={February},
  url = {https://arxiv.org/pdf/2211.06642.pdf}
} 

@article{Dalvi_Nortonsmith_Bau_Belinkov_Sajjad_Durrani_Glass_2019, title={NeuroX: A Toolkit for Analyzing Individual Neurons in Neural Networks}, volume={33}, url={https://ojs.aaai.org/index.php/AAAI/article/view/5063}, DOI={10.1609/aaai.v33i01.33019851}, abstractNote={&lt;p&gt;We present a toolkit to facilitate the interpretation and understanding of neural network models. The toolkit provides several methods to identify salient neurons with respect to the model itself or an external task. A user can visualize selected neurons, ablate them to measure their effect on the model accuracy, and manipulate them to control the behavior of the model at the test time. Such an analysis has a potential to serve as a springboard in various research directions, such as understanding the model, better architectural choices, model distillation and controlling data biases. The toolkit is available for download.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Dalvi, Fahim and Nortonsmith, Avery and Bau, Anthony and Belinkov, Yonatan and Sajjad, Hassan and Durrani, Nadir and Glass, James}, year={2019}, month={Jul.}, pages={9851-9852} }

@inproceedings{liepins-etal-2017-summa,
    title = "The {SUMMA} Platform Prototype",
    author = "Liepins, Renars  and
      Germann, Ulrich  and
      Barzdins, Guntis  and
      Birch, Alexandra  and
      Renals, Steve  and
      Weber, Susanne  and
      van der Kreeft, Peggy  and
      Bourlard, Herv{\'e}  and
      Prieto, Jo{\~a}o  and
      Klejch, Ond{\v{r}}ej  and
      Bell, Peter  and
      Lazaridis, Alexandros  and
      Mendes, Alfonso  and
      Riedel, Sebastian  and
      Almeida, Mariana S. C.  and
      Balage, Pedro  and
      Cohen, Shay B.  and
      Dwojak, Tomasz  and
      Garner, Philip N.  and
      Giefer, Andreas  and
      Junczys-Dowmunt, Marcin  and
      Imran, Hina  and
      Nogueira, David  and
      Ali, Ahmed  and
      Miranda, Sebasti{\~a}o  and
      Popescu-Belis, Andrei  and
      Miculicich Werlen, Lesly  and
      Papasarantopoulos, Nikos  and
      Obamuyide, Abiola  and
      Jones, Clive  and
      Dalvi, Fahim  and
      Vlachos, Andreas  and
      Wang, Yang  and
      Tong, Sibo  and
      Sennrich, Rico  and
      Pappas, Nikolaos  and
      Narayan, Shashi  and
      Damonte, Marco  and
      Durrani, Nadir  and
      Khurana, Sameer  and
      Abdelali, Ahmed  and
      Sajjad, Hassan  and
      Vogel, Stephan  and
      Sheppey, David  and
      Hernon, Chris  and
      Mitchell, Jeff",
    editor = "Martins, Andr{\'e}  and
      Pe{\~n}as, Anselmo",
    booktitle = "Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-3029",
    pages = "116--119",
    abstract = "We present the first prototype of the SUMMA Platform: an integrated platform for multilingual media monitoring. The platform contains a rich suite of low-level and high-level natural language processing technologies: automatic speech recognition of broadcast media, machine translation, automated tagging and classification of named entities, semantic parsing to detect relationships between entities, and automatic construction / augmentation of factual knowledge bases. Implemented on the Docker platform, it can easily be deployed, customised, and scaled to large volumes of incoming media streams.",
}

@inproceedings{dalvi-etal-2017-qcri,
    title = "{QCRI} Live Speech Translation System",
    author = "Dalvi, Fahim  and
      Zhang, Yifan  and
      Khurana, Sameer  and
      Durrani, Nadir  and
      Sajjad, Hassan  and
      Abdelali, Ahmed  and
      Mubarak, Hamdy  and
      Ali, Ahmed  and
      Vogel, Stephan",
    editor = "Martins, Andr{\'e}  and
      Pe{\~n}as, Anselmo",
    booktitle = "Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E17-3016",
    pages = "61--64",
    abstract = "This paper presents QCRI{'}s Arabic-to-English live speech translation system. It features modern web technologies to capture live audio, and broadcasts Arabic transcriptions and English translations simultaneously. Our Kaldi-based ASR system uses the Time Delay Neural Network (TDNN) architecture, while our Machine Translation (MT) system uses both phrase-based and neural frameworks. Although our neural MT system is slower than the phrase-based system, it produces significantly better translations and is memory efficient. The demo is available at \url{https://st.qcri.org/demos/livetranslation}.",
}

@inproceedings{mubarak-etal-2019-system,
    title = "A System for Diacritizing Four Varieties of {A}rabic",
    author = "Mubarak, Hamdy  and
      Abdelali, Ahmed  and
      Darwish, Kareem  and
      Eldesouki, Mohamed  and
      Samih, Younes  and
      Sajjad, Hassan",
    editor = "Pad{\'o}, Sebastian  and
      Huang, Ruihong",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-3037",
    doi = "10.18653/v1/D19-3037",
    pages = "217--222",
    abstract = "Short vowels, aka diacritics, are more often omitted when writing different varieties of Arabic including Modern Standard Arabic (MSA), Classical Arabic (CA), and Dialectal Arabic (DA). However, diacritics are required to properly pronounce words, which makes diacritic restoration (a.k.a. diacritization) essential for language learning and text-to-speech applications. In this paper, we present a system for diacritizing MSA, CA, and two varieties of DA, namely Moroccan and Tunisian. The system uses a character level sequence-to-sequence deep learning model that requires no feature engineering and beats all previous SOTA systems for all the Arabic varieties that we test on.",
}